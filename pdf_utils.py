from PyPDF2 import PdfReader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_groq import ChatGroq  # Correct class for Groq LLM
import os
from dotenv import load_dotenv

load_dotenv()

def extract_text_from_pdf(pdf_path: str) -> str:
    """
    Loads a PDF file and returns the full extracted text from all pages.
    Args:
        pdf_path (str): Path to the PDF file.
    Returns:
        str: Concatenated text from all pages.
    """
    reader = PdfReader(pdf_path)
    text = ""
    for page in reader.pages:
        page_text = page.extract_text()
        if page_text:
            text += page_text
    return text

def split_text_with_overlap(text: str, chunk_size: int = 500, overlap: int = 100) -> list[str]:
    """
    Splits text into chunks with specified size and overlap for context retention.
    Args:
        text (str): The input text to split.
        chunk_size (int): The size of each chunk (default 500).
        overlap (int): The number of overlapping characters between chunks (default 100).
    Returns:
        list[str]: List of text chunks.
    """
    if chunk_size <= overlap:
        raise ValueError("chunk_size must be greater than overlap")
    chunks = []
    start = 0
    text_length = len(text)
    while start < text_length:
        end = min(start + chunk_size, text_length)
        chunks.append(text[start:end])
        start += chunk_size - overlap
    return chunks

def store_chunks_in_faiss(
    chunks: list[str]
) -> FAISS:
    """
    Stores text chunks in a FAISS vector store using HuggingFace embeddings via LangChain.
    Args:
        chunks (list[str]): List of text chunks.
    Returns:
        FAISS: The FAISS vector store object.
    """
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    db = FAISS.from_texts(chunks, embeddings)
    return db

def query_pdf_with_retrieval_qa(
    query: str,
    vector_store: FAISS,
    groq_api_key: str = None,
    temperature: float = 0.0
) -> str:
    """
    Takes a user query, performs similarity search on the vector store, and returns the result from ChatGroq LLM using LangChain RetrievalQA.
    Args:
        query (str): The user's question.
        vector_store (FAISS): The FAISS vector store.
        groq_api_key (str): Required for Groq LLM.
        temperature (float): LLM temperature (default 0.0).
    Returns:
        str: The answer generated by the LLM.
    """
    api_key = groq_api_key or os.getenv("GROQ_API_KEY")
    model_name = os.getenv("MODEL_NAME", "openai/gpt-oss-120b")
    
    if not api_key:
        raise ValueError("Groq API key must be provided via argument or environment variable.")
        
    llm = ChatGroq(
        api_key=api_key,
        model=model_name,
        temperature=temperature
    )
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vector_store.as_retriever(),
    )
    return qa.run(query) 